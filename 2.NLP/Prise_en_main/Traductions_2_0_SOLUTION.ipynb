{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traductions 2.0 - SOLUTION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NFuUYKHABlD",
        "colab_type": "text"
      },
      "source": [
        "# Traductions 2.0 - Neural Machine Translation\n",
        "\n",
        "Selon le papier de Google [*Attention is all you need*](https://arxiv.org/abs/1706.03762), vous n'avez besoin que de couches d'Attention pour faire comprendre à un modèle de Deep Learning la complexité d'une phrase. Nous allons essayer d'implémenter ce type de modèle pour notre traducteur. \n",
        "\n",
        "## Description du projet \n",
        "\n",
        "Pour ce projet, nous allons pouvoir reprendre le preprocessing que nous avions fait précedemment. A la seule précision que celui-ci sera simplifié. \n",
        "\n",
        "### Import des données \n",
        "\n",
        "Vous aurez le même fichier `.txt` contenant une phrase avec sa traduction séparée par une tabulation (`\\t`). Vous devrez donc importer ces données et les lire via `pandas` ou `numpy`. \n",
        "\n",
        "Vos données se trouvent sur ce lien : https://go.aws/38ECHUB\n",
        "\n",
        "### Preprocessing \n",
        "\n",
        "Tout l'objectif de votre preprocessing est d'arriver à exprimer votre phrase d'entrée (française) en une séquence d'indices.\n",
        "\n",
        "i.e :\n",
        "\n",
        "* je suis malade ---> `[123, 21, 34, 0, 0, 0, 0]`\n",
        "\n",
        "Ce qui donne une *shape* -> `(batch_size, max_len_of_a_french_sentence)`\n",
        "\n",
        "Les indices correspondent à un numéro que vous devrez attribuer pour chaque token de mots. \n",
        "\n",
        "Les zéros correspondent à ce qu'on appelle des [*padded_sequences*](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) qui permettent le fait que tous les séquences de mots aient la même longueur (obligatoire pour votre algorithme). \n",
        "\n",
        "Cette fois, vous n'aurez pas à *one hot encoder* votre variable cible. Vous pourrez simplement créer un vecteur similaire à celui de votre phrase d'entrée. \n",
        "\n",
        "i.e : \n",
        "\n",
        "* I am sick ---> `[43, 2, 42, 0, 0, 0]`\n",
        "\n",
        "ATTENTION, vous aurez cependant besoin d'ajouter une étape dans votre preprocessing. Pour chacune des phrases, vous aurez besoin d'ajouter un token `<start>` & `<end>` pour indiquer le début et la fin d'une phrase. Vous pourrez le faire via `Spacy`\n",
        "\n",
        "Pour aider dans votre tâche, vous pourrez utiliser : \n",
        "\n",
        "* `Pandas` ou `Numpy` pour la lecture du fichier text\n",
        "* `Spacy` pour la Tokenisation \n",
        "* `Tensorflow` pour le [padded_sequence](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) \n",
        "\n",
        "### Modélisation \n",
        "\n",
        "Pour la modélisation, vous aurez besoin de mettre en place des couches d'attention. Vous devrez : \n",
        "\n",
        "* Créer une classe `Encoder` qui hérite de `tf.keras.Model`\n",
        "* Créer une couche d'Attention Bahdanau qui va être une classe qui hérite de `tf.keras.layers.Layer`\n",
        "* Créer enfin une classe `Decoder` qui hérite de `tf.keras.Model`\n",
        "\n",
        "\n",
        "Vous devrez créer votre propre fonction de coût ainsi que votre propre boucle d'entrainement. \n",
        "\n",
        "\n",
        "### Conseils \n",
        "\n",
        "Ne prenez pas l'entièreté du dataset au départ pour vos expérimentations, prenez simplement 5000 voire même 3000 phrases. Cela vous permettra d'itérer plus vite et d'éviter des bugs liés simplement à votre besoin en puissance de calcul. \n",
        "\n",
        "Aussi, vous pouvez vous inspirer du tutoriel [Neural Machine Translation with Attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention) de TensorFlow. \n",
        "\n",
        "Good Luck !\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MAiXgYbAVLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "outputId": "45368e3d-b374-42ba-d6a4-443ac0e9b569"
      },
      "source": [
        "!pip install --upgrade tensorflow "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qUhyNPnhBtk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "249b5b52-0f6e-43f5-a873-4a4d20fe67b1"
      },
      "source": [
        "# Import des librairies nécessaires\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf \n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSWse8KgHmQw",
        "colab_type": "text"
      },
      "source": [
        "## Import des données "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2-Sd6lq_8ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction de chargement du document txt\n",
        "def load_doc(url):\n",
        "  df = pd.read_csv(\"https://go.aws/38ECHUB\", delimiter=\"\\t\", header=None)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj34GLiihGw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "c7ec1440-8799-443e-c96f-ae80d4b8d746"
      },
      "source": [
        "# Chargement du document txt\n",
        "doc = load_doc(\"https://go.aws/38ECHUB\")\n",
        "doc.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0           1\n",
              "0   Go.        Va !\n",
              "1   Hi.     Salut !\n",
              "2  Run!     Cours !\n",
              "3  Run!    Courez !\n",
              "4  Wow!  Ça alors !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrcFSfBZMuQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prenons simplement un sample de 5000 phrases pour éviter des lenteurs \n",
        "doc = doc.sample(5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z1Ih3M2jVr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add a <start> and <end> token \n",
        "def begin_end_sentence(sentence):\n",
        "  sentence = \"<start> \"+ sentence + \" <end>\"\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVMl6744jmrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add <start> and <end> token\n",
        "doc.iloc[:, 0] = doc.iloc[:, 0].apply(lambda x: begin_end_sentence(x))\n",
        "#doc.iloc[:, 1] = doc.iloc[:, 1].apply(lambda x: begin_end_sentence(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0gGTgDFhJN6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ef90be9-482c-4ea7-ed43-0f26b4f97f86"
      },
      "source": [
        "# Chargement des langages français et anglais de spacy \n",
        "!python -m spacy download fr_core_news_md\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.2.5/fr_core_news_md-2.2.5.tar.gz (88.6MB)\n",
            "\u001b[K     |████████████████████████████████| 88.6MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: fr-core-news-md\n",
            "  Building wheel for fr-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-md: filename=fr_core_news_md-2.2.5-cp36-none-any.whl size=90338488 sha256=55a44fcb29cc9cc78532545a7cb4a10b92cdb0d22ad741d99bc96883dd15c21e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jbfs0xst/wheels/c6/18/b6/f628642acc7872a53cf81269dd1c394d96da69564ccfac5425\n",
            "Successfully built fr-core-news-md\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_md')\n",
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=e56bb9e5aa6732781e114b8cc4280aa36455e0ede19fd2e0076c92af4784d801\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p6iru9c1/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBPt41Isna8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import de chacun des langages\n",
        "import fr_core_news_md\n",
        "import en_core_web_md\n",
        "nlp_fr = fr_core_news_md.load()\n",
        "nlp_en = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE7sn2ftlfJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add <start> & <end> special case\n",
        "from spacy.symbols import ORTH\n",
        "\n",
        "start_case = [{ORTH:\"<start>\"}]\n",
        "end_case = [{ORTH: \"<end>\"}]\n",
        "\n",
        "#nlp_fr.tokenizer.add_special_case(\"<start>\", start_case)\n",
        "#nlp_fr.tokenizer.add_special_case(\"<end>\", end_case)\n",
        "\n",
        "nlp_en.tokenizer.add_special_case(\"<start>\", start_case)\n",
        "nlp_en.tokenizer.add_special_case(\"<end>\", end_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O4hj8DGoFzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement du corpus entier de phrases françaises et anglaises\n",
        "fr_corpus = \" \".join(doc.iloc[:, 1].to_list())\n",
        "en_corpus = \" \".join(doc.iloc[:, 0].to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPzvDSUWoWfW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a1708c75-72c6-4d33-c46d-79344753180b"
      },
      "source": [
        "# Chargement des deux corpus dans spacy \n",
        "%%time\n",
        "import time\n",
        "nlp_fr.max_length = len(fr_corpus)\n",
        "nlp_en.max_length = len(en_corpus)\n",
        "\n",
        "fr_doc = nlp_fr(fr_corpus)\n",
        "en_doc = nlp_en(en_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13.2 s, sys: 522 ms, total: 13.7 s\n",
            "Wall time: 13.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBu43uJ77Ad6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e7f54e9f-f0bc-4bec-864d-54df725ea0c6"
      },
      "source": [
        "# Tokenisation de chacune des phrases via spacy \n",
        "%%time\n",
        "doc[\"fr_tokens\"] = doc.iloc[:, 1].apply(lambda x: nlp_fr.tokenizer(x))\n",
        "doc[\"en_tokens\"] = doc.iloc[:, 0].apply(lambda x: nlp_en.tokenizer(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 362 ms, sys: 14 ms, total: 376 ms\n",
            "Wall time: 375 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQQOU7OI7-xV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "752b7034-9847-4869-e206-e8c63c1969e5"
      },
      "source": [
        "doc.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>fr_tokens</th>\n",
              "      <th>en_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146434</th>\n",
              "      <td>&lt;start&gt; He works as a teacher, but actually he...</td>\n",
              "      <td>Il travaille comme enseignant, mais en fait c'...</td>\n",
              "      <td>(Il, travaille, comme, enseignant, ,, mais, en...</td>\n",
              "      <td>(&lt;start&gt;, He, works, as, a, teacher, ,, but, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40205</th>\n",
              "      <td>&lt;start&gt; Let's take a breather. &lt;end&gt;</td>\n",
              "      <td>Prenons un moment de repos.</td>\n",
              "      <td>(Prenons, un, moment, de, repos, .)</td>\n",
              "      <td>(&lt;start&gt;, Let, 's, take, a, breather, ., &lt;end&gt;)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131048</th>\n",
              "      <td>&lt;start&gt; I didn't even know Tom had a girlfrien...</td>\n",
              "      <td>Je ne savais même pas que Tom avait une petite...</td>\n",
              "      <td>(Je, ne, savais, même, pas, que, Tom, avait, u...</td>\n",
              "      <td>(&lt;start&gt;, I, did, n't, even, know, Tom, had, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131559</th>\n",
              "      <td>&lt;start&gt; I wanted to talk to you about somethin...</td>\n",
              "      <td>Je voulais m'entretenir avec vous de quelque c...</td>\n",
              "      <td>(Je, voulais, m', entretenir, avec, vous, de, ...</td>\n",
              "      <td>(&lt;start&gt;, I, wanted, to, talk, to, you, about,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43753</th>\n",
              "      <td>&lt;start&gt; He is a lovable person. &lt;end&gt;</td>\n",
              "      <td>C'est une personne adorable.</td>\n",
              "      <td>(C', est, une, personne, adorable, .)</td>\n",
              "      <td>(&lt;start&gt;, He, is, a, lovable, person, ., &lt;end&gt;)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        0  ...                                          en_tokens\n",
              "146434  <start> He works as a teacher, but actually he...  ...  (<start>, He, works, as, a, teacher, ,, but, a...\n",
              "40205                <start> Let's take a breather. <end>  ...    (<start>, Let, 's, take, a, breather, ., <end>)\n",
              "131048  <start> I didn't even know Tom had a girlfrien...  ...  (<start>, I, did, n't, even, know, Tom, had, a...\n",
              "131559  <start> I wanted to talk to you about somethin...  ...  (<start>, I, wanted, to, talk, to, you, about,...\n",
              "43753               <start> He is a lovable person. <end>  ...    (<start>, He, is, a, lovable, person, ., <end>)\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwCN5z21xo5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fd66835-ba0d-48d3-a065-0373dc143ceb"
      },
      "source": [
        "# Création d'un set() qui va prendre tous les tokens unique de notre corpus de texte\n",
        "en_tokens = [token.text for token in en_doc]\n",
        "en_vocabulary_set= set(en_tokens)\n",
        "en_vocab_size = len(en_vocabulary_set)\n",
        "print(en_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIwlGhNDykzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "899eaeec-4f4b-4cdd-ae8c-efef95b74cb6"
      },
      "source": [
        "# Même chose pour le français \n",
        "fr_tokens = [token.text for token in fr_doc]\n",
        "fr_vocabulary_set= set(fr_tokens)\n",
        "fr_vocab_size = len(fr_vocabulary_set)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUjLaIjB0e-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "bff2e844-b3e4-40d1-ef87-a96de1997d4a"
      },
      "source": [
        "en_tokens[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start>',\n",
              " 'I',\n",
              " \"'ve\",\n",
              " 'already',\n",
              " 'written',\n",
              " 'my',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'report']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6miVbPY0xZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "1d758ed3-0e11-40b4-98da-33aab6bf8a89"
      },
      "source": [
        "[word for word in en_vocabulary_set][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['winter',\n",
              " 'related',\n",
              " 'good',\n",
              " 'raised',\n",
              " 'waited',\n",
              " 'singing',\n",
              " 'fit',\n",
              " 'earth',\n",
              " 'holding',\n",
              " 'leads']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy_ou60_3X4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création d'un id pour chacun des tokens\n",
        "all_en_tokens = {}\n",
        "for i,en_token in enumerate(en_vocabulary_set):\n",
        "  all_en_tokens[en_token] = i+1 # On prend à i+1 pour laisser la valeur 0 pour la création des padded_sequences\n",
        "\n",
        "all_fr_tokens = {}\n",
        "for i, fr_token in enumerate(fr_vocabulary_set):\n",
        "  all_fr_tokens[fr_token] = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFIfUb1i7ia3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création de fonction qui vont créer un vecteur d'indices pour chacune des séquences de tokens\n",
        "def en_tokens_to_index(tokens):\n",
        "  indices = []\n",
        "  for token in tokens:\n",
        "    indices.append(all_en_tokens[token.text])\n",
        "  \n",
        "  return indices\n",
        "\n",
        "def fr_tokens_to_index(tokens):\n",
        "  indices = []\n",
        "  for token in tokens:\n",
        "    indices.append(all_fr_tokens[token.text])\n",
        "  \n",
        "  return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA8Gc2Yw9abI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformation des tokens en indices\n",
        "doc[\"fr_indices\"] = doc[\"fr_tokens\"].apply(lambda x: fr_tokens_to_index(x))\n",
        "doc[\"en_indices\"] = doc[\"en_tokens\"].apply(lambda x: en_tokens_to_index(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIWHfQsj-Yn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "14eeeb6a-c69f-4671-b462-b3d236d019a5"
      },
      "source": [
        "doc.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>fr_tokens</th>\n",
              "      <th>en_tokens</th>\n",
              "      <th>fr_indices</th>\n",
              "      <th>en_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146434</th>\n",
              "      <td>&lt;start&gt; He works as a teacher, but actually he...</td>\n",
              "      <td>Il travaille comme enseignant, mais en fait c'...</td>\n",
              "      <td>(Il, travaille, comme, enseignant, ,, mais, en...</td>\n",
              "      <td>(&lt;start&gt;, He, works, as, a, teacher, ,, but, a...</td>\n",
              "      <td>[3616, 3952, 217, 3882, 1552, 1867, 2515, 1713...</td>\n",
              "      <td>[1238, 2259, 585, 3239, 3081, 2810, 1129, 2992...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40205</th>\n",
              "      <td>&lt;start&gt; Let's take a breather. &lt;end&gt;</td>\n",
              "      <td>Prenons un moment de repos.</td>\n",
              "      <td>(Prenons, un, moment, de, repos, .)</td>\n",
              "      <td>(&lt;start&gt;, Let, 's, take, a, breather, ., &lt;end&gt;)</td>\n",
              "      <td>[2341, 777, 4670, 3155, 3571, 871]</td>\n",
              "      <td>[1238, 3010, 1501, 1803, 3081, 194, 603, 1259]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131048</th>\n",
              "      <td>&lt;start&gt; I didn't even know Tom had a girlfrien...</td>\n",
              "      <td>Je ne savais même pas que Tom avait une petite...</td>\n",
              "      <td>(Je, ne, savais, même, pas, que, Tom, avait, u...</td>\n",
              "      <td>(&lt;start&gt;, I, did, n't, even, know, Tom, had, a...</td>\n",
              "      <td>[4390, 458, 2702, 3158, 1392, 1254, 2826, 3453...</td>\n",
              "      <td>[1238, 1031, 3100, 2339, 1657, 2517, 2087, 144...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131559</th>\n",
              "      <td>&lt;start&gt; I wanted to talk to you about somethin...</td>\n",
              "      <td>Je voulais m'entretenir avec vous de quelque c...</td>\n",
              "      <td>(Je, voulais, m', entretenir, avec, vous, de, ...</td>\n",
              "      <td>(&lt;start&gt;, I, wanted, to, talk, to, you, about,...</td>\n",
              "      <td>[4390, 510, 3063, 1273, 2576, 4231, 3155, 2090...</td>\n",
              "      <td>[1238, 1031, 3391, 422, 1937, 422, 881, 1372, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43753</th>\n",
              "      <td>&lt;start&gt; He is a lovable person. &lt;end&gt;</td>\n",
              "      <td>C'est une personne adorable.</td>\n",
              "      <td>(C', est, une, personne, adorable, .)</td>\n",
              "      <td>(&lt;start&gt;, He, is, a, lovable, person, ., &lt;end&gt;)</td>\n",
              "      <td>[1126, 2313, 4577, 886, 4334, 871]</td>\n",
              "      <td>[1238, 2259, 1015, 3081, 3337, 2895, 603, 1259]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        0  ...                                         en_indices\n",
              "146434  <start> He works as a teacher, but actually he...  ...  [1238, 2259, 585, 3239, 3081, 2810, 1129, 2992...\n",
              "40205                <start> Let's take a breather. <end>  ...     [1238, 3010, 1501, 1803, 3081, 194, 603, 1259]\n",
              "131048  <start> I didn't even know Tom had a girlfrien...  ...  [1238, 1031, 3100, 2339, 1657, 2517, 2087, 144...\n",
              "131559  <start> I wanted to talk to you about somethin...  ...  [1238, 1031, 3391, 422, 1937, 422, 881, 1372, ...\n",
              "43753               <start> He is a lovable person. <end>  ...    [1238, 2259, 1015, 3081, 3337, 2895, 603, 1259]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgFwiD7TQXHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création d'une fonction qui va compter la longueur maximum d'une phrase\n",
        "def max_len(lines):\n",
        "  return max(len(line) for line in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PppvMC4K7J7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Application de la fonction sur les tokens français et anglais \n",
        "fr_max_len = max_len(doc['fr_indices'].to_list())\n",
        "en_max_len = max_len(doc['en_indices'].to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSi2j-6I7K5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4ee63423-3048-40d8-bb9b-08a09ecaacdd"
      },
      "source": [
        "# Utilisation de Keras pour créer des séquences de tokens de la même longueur\n",
        "%%time\n",
        "padded_fr_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"fr_indices\"], maxlen=fr_max_len, padding=\"post\")\n",
        "padded_en_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"en_indices\"], maxlen=en_max_len, padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 53.7 ms, sys: 75 µs, total: 53.8 ms\n",
            "Wall time: 52.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEBeXGQyvaeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "515c7fdd-c511-473e-90ec-9daf4b1ab9f5"
      },
      "source": [
        "padded_en_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1238, 1031,  175, ...,    0,    0,    0],\n",
              "       [1238, 1031, 2959, ...,    0,    0,    0],\n",
              "       [1238, 1031, 1125, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [1238, 1031, 3100, ...,    0,    0,    0],\n",
              "       [1238, 1031, 3391, ...,    0,    0,    0],\n",
              "       [1238, 2259, 1015, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueIigsED1nWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création de variables que l'on va réutiliser pour nos modèles\n",
        "BATCH_SIZE = 64\n",
        "TAKE_SIZE = int(0.7*len(doc)/BATCH_SIZE)\n",
        "BUFFER_SIZE = TAKE_SIZE * BATCH_SIZE\n",
        "steps_per_epoch = TAKE_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = fr_vocab_size\n",
        "vocab_tar_size = en_vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZOn0SAFB5eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a tensorflow dataset complet\n",
        "tf_ds = tf.data.Dataset.from_tensor_slices((padded_fr_indices, padded_en_indices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0aIpW5cZmnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle & Batch\n",
        "tf_ds = tf_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCrDIAkMZPUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Test Split\n",
        "train_data = tf_ds.take(TAKE_SIZE)\n",
        "test_data = tf_ds.skip(TAKE_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmEescP6jSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a0874a6d-11e9-481a-ecb3-943d1d660cdf"
      },
      "source": [
        "input_text, output_text = next(iter(train_data))\n",
        "print(input_text.numpy().shape)\n",
        "print(output_text.numpy().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 28)\n",
            "(64, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbbOi7sYVDU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder \n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cmD6jUP74Rx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "96c40153-148f-445e-b2c6-e8cb4ab517fb"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size +1, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# Echantillon d'output\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(input_text, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 28, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq2nP5l_76LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # Ceci est fait pour pour calculer notre score \"d'attention\"\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # On obtient 1 sur le dernier axe car on applique le score à self.V\n",
        "    # La shape du tenseur avant que l'on applique self.V est (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KetnxQvd8bF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "93909cb7-82c3-4d29-e7b3-03bf105ca6cc"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruo6sIk98eLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Utilisé pour attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape après embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape après concaténation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # Passage du vecteur concaténé à la couche GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPW5WmrY8hyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "229c53a9-c5d8-45a1-bc27-616f5e67c640"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size + 1, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 3524)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2h8jDTT8wHq",
        "colab_type": "text"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ldPbErh8j1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBFavAT78w25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8V4m-De84om",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUAgTA4-8zsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([all_en_tokens[\"<start>\"]] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgoAc_Sd85tt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8846e050-3fe4-4871-b693-0703b2606330"
      },
      "source": [
        "EPOCHS = 30\n",
        "steps_per_epoch = TAKE_SIZE\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_data.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.4566\n",
            "Epoch 1 Batch 10 Loss 1.8106\n",
            "Epoch 1 Batch 20 Loss 1.8215\n",
            "Epoch 1 Batch 30 Loss 1.7302\n",
            "Epoch 1 Batch 40 Loss 1.6291\n",
            "Epoch 1 Batch 50 Loss 1.5803\n",
            "Epoch 1 Loss 1.7505\n",
            "Time taken for 1 epoch 48.712414026260376 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5499\n",
            "Epoch 2 Batch 10 Loss 1.4397\n",
            "Epoch 2 Batch 20 Loss 1.3814\n",
            "Epoch 2 Batch 30 Loss 1.3921\n",
            "Epoch 2 Batch 40 Loss 1.5381\n",
            "Epoch 2 Batch 50 Loss 1.3987\n",
            "Epoch 2 Loss 1.4542\n",
            "Time taken for 1 epoch 8.688588619232178 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3288\n",
            "Epoch 3 Batch 10 Loss 1.3104\n",
            "Epoch 3 Batch 20 Loss 1.2824\n",
            "Epoch 3 Batch 30 Loss 1.2887\n",
            "Epoch 3 Batch 40 Loss 1.2676\n",
            "Epoch 3 Batch 50 Loss 1.2518\n",
            "Epoch 3 Loss 1.3216\n",
            "Time taken for 1 epoch 8.300629138946533 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.2007\n",
            "Epoch 4 Batch 10 Loss 1.1316\n",
            "Epoch 4 Batch 20 Loss 1.2264\n",
            "Epoch 4 Batch 30 Loss 1.2310\n",
            "Epoch 4 Batch 40 Loss 1.1657\n",
            "Epoch 4 Batch 50 Loss 1.2415\n",
            "Epoch 4 Loss 1.2058\n",
            "Time taken for 1 epoch 9.120444536209106 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2286\n",
            "Epoch 5 Batch 10 Loss 1.1383\n",
            "Epoch 5 Batch 20 Loss 1.1293\n",
            "Epoch 5 Batch 30 Loss 1.1527\n",
            "Epoch 5 Batch 40 Loss 1.1135\n",
            "Epoch 5 Batch 50 Loss 1.1327\n",
            "Epoch 5 Loss 1.1261\n",
            "Time taken for 1 epoch 8.281737804412842 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.9655\n",
            "Epoch 6 Batch 10 Loss 1.0979\n",
            "Epoch 6 Batch 20 Loss 1.1330\n",
            "Epoch 6 Batch 30 Loss 1.0663\n",
            "Epoch 6 Batch 40 Loss 1.0444\n",
            "Epoch 6 Batch 50 Loss 1.1925\n",
            "Epoch 6 Loss 1.0565\n",
            "Time taken for 1 epoch 8.631457567214966 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9761\n",
            "Epoch 7 Batch 10 Loss 1.0933\n",
            "Epoch 7 Batch 20 Loss 0.9781\n",
            "Epoch 7 Batch 30 Loss 0.9242\n",
            "Epoch 7 Batch 40 Loss 1.1643\n",
            "Epoch 7 Batch 50 Loss 1.0071\n",
            "Epoch 7 Loss 0.9814\n",
            "Time taken for 1 epoch 8.332617282867432 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8113\n",
            "Epoch 8 Batch 10 Loss 0.8678\n",
            "Epoch 8 Batch 20 Loss 0.8606\n",
            "Epoch 8 Batch 30 Loss 0.9066\n",
            "Epoch 8 Batch 40 Loss 0.9867\n",
            "Epoch 8 Batch 50 Loss 0.9147\n",
            "Epoch 8 Loss 0.9032\n",
            "Time taken for 1 epoch 8.682822704315186 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8744\n",
            "Epoch 9 Batch 10 Loss 0.8182\n",
            "Epoch 9 Batch 20 Loss 0.8665\n",
            "Epoch 9 Batch 30 Loss 0.8566\n",
            "Epoch 9 Batch 40 Loss 0.7691\n",
            "Epoch 9 Batch 50 Loss 0.8627\n",
            "Epoch 9 Loss 0.8351\n",
            "Time taken for 1 epoch 8.299999952316284 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.7619\n",
            "Epoch 10 Batch 10 Loss 0.8459\n",
            "Epoch 10 Batch 20 Loss 0.7264\n",
            "Epoch 10 Batch 30 Loss 0.8055\n",
            "Epoch 10 Batch 40 Loss 0.7048\n",
            "Epoch 10 Batch 50 Loss 0.7858\n",
            "Epoch 10 Loss 0.7614\n",
            "Time taken for 1 epoch 8.570255756378174 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6380\n",
            "Epoch 11 Batch 10 Loss 0.6438\n",
            "Epoch 11 Batch 20 Loss 0.6801\n",
            "Epoch 11 Batch 30 Loss 0.6912\n",
            "Epoch 11 Batch 40 Loss 0.6796\n",
            "Epoch 11 Batch 50 Loss 0.7184\n",
            "Epoch 11 Loss 0.6796\n",
            "Time taken for 1 epoch 8.29270887374878 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.5690\n",
            "Epoch 12 Batch 10 Loss 0.5757\n",
            "Epoch 12 Batch 20 Loss 0.5574\n",
            "Epoch 12 Batch 30 Loss 0.6655\n",
            "Epoch 12 Batch 40 Loss 0.6405\n",
            "Epoch 12 Batch 50 Loss 0.6359\n",
            "Epoch 12 Loss 0.6133\n",
            "Time taken for 1 epoch 8.607705116271973 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.4780\n",
            "Epoch 13 Batch 10 Loss 0.5764\n",
            "Epoch 13 Batch 20 Loss 0.4994\n",
            "Epoch 13 Batch 30 Loss 0.5456\n",
            "Epoch 13 Batch 40 Loss 0.5031\n",
            "Epoch 13 Batch 50 Loss 0.6182\n",
            "Epoch 13 Loss 0.5303\n",
            "Time taken for 1 epoch 8.276198387145996 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.3659\n",
            "Epoch 14 Batch 10 Loss 0.3552\n",
            "Epoch 14 Batch 20 Loss 0.4921\n",
            "Epoch 14 Batch 30 Loss 0.4827\n",
            "Epoch 14 Batch 40 Loss 0.4659\n",
            "Epoch 14 Batch 50 Loss 0.4687\n",
            "Epoch 14 Loss 0.4690\n",
            "Time taken for 1 epoch 8.576748132705688 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.3876\n",
            "Epoch 15 Batch 10 Loss 0.4548\n",
            "Epoch 15 Batch 20 Loss 0.3397\n",
            "Epoch 15 Batch 30 Loss 0.4550\n",
            "Epoch 15 Batch 40 Loss 0.3776\n",
            "Epoch 15 Batch 50 Loss 0.3967\n",
            "Epoch 15 Loss 0.4086\n",
            "Time taken for 1 epoch 8.351473808288574 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3400\n",
            "Epoch 16 Batch 10 Loss 0.3498\n",
            "Epoch 16 Batch 20 Loss 0.3071\n",
            "Epoch 16 Batch 30 Loss 0.3705\n",
            "Epoch 16 Batch 40 Loss 0.3782\n",
            "Epoch 16 Batch 50 Loss 0.3482\n",
            "Epoch 16 Loss 0.3570\n",
            "Time taken for 1 epoch 8.6280198097229 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.3462\n",
            "Epoch 17 Batch 10 Loss 0.2823\n",
            "Epoch 17 Batch 20 Loss 0.2897\n",
            "Epoch 17 Batch 30 Loss 0.3150\n",
            "Epoch 17 Batch 40 Loss 0.3248\n",
            "Epoch 17 Batch 50 Loss 0.3216\n",
            "Epoch 17 Loss 0.3115\n",
            "Time taken for 1 epoch 8.247327327728271 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2263\n",
            "Epoch 18 Batch 10 Loss 0.2658\n",
            "Epoch 18 Batch 20 Loss 0.2165\n",
            "Epoch 18 Batch 30 Loss 0.2728\n",
            "Epoch 18 Batch 40 Loss 0.2516\n",
            "Epoch 18 Batch 50 Loss 0.3474\n",
            "Epoch 18 Loss 0.2631\n",
            "Time taken for 1 epoch 8.53091549873352 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1977\n",
            "Epoch 19 Batch 10 Loss 0.1871\n",
            "Epoch 19 Batch 20 Loss 0.1901\n",
            "Epoch 19 Batch 30 Loss 0.2760\n",
            "Epoch 19 Batch 40 Loss 0.2820\n",
            "Epoch 19 Batch 50 Loss 0.2576\n",
            "Epoch 19 Loss 0.2340\n",
            "Time taken for 1 epoch 8.253600597381592 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.2139\n",
            "Epoch 20 Batch 10 Loss 0.2387\n",
            "Epoch 20 Batch 20 Loss 0.1995\n",
            "Epoch 20 Batch 30 Loss 0.2171\n",
            "Epoch 20 Batch 40 Loss 0.1723\n",
            "Epoch 20 Batch 50 Loss 0.2107\n",
            "Epoch 20 Loss 0.1989\n",
            "Time taken for 1 epoch 8.658693552017212 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1594\n",
            "Epoch 21 Batch 10 Loss 0.1675\n",
            "Epoch 21 Batch 20 Loss 0.1713\n",
            "Epoch 21 Batch 30 Loss 0.1897\n",
            "Epoch 21 Batch 40 Loss 0.2061\n",
            "Epoch 21 Batch 50 Loss 0.1816\n",
            "Epoch 21 Loss 0.1653\n",
            "Time taken for 1 epoch 8.324267148971558 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0977\n",
            "Epoch 22 Batch 10 Loss 0.1000\n",
            "Epoch 22 Batch 20 Loss 0.1499\n",
            "Epoch 22 Batch 30 Loss 0.1688\n",
            "Epoch 22 Batch 40 Loss 0.1608\n",
            "Epoch 22 Batch 50 Loss 0.1137\n",
            "Epoch 22 Loss 0.1440\n",
            "Time taken for 1 epoch 8.572293043136597 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0992\n",
            "Epoch 23 Batch 10 Loss 0.1324\n",
            "Epoch 23 Batch 20 Loss 0.1182\n",
            "Epoch 23 Batch 30 Loss 0.1418\n",
            "Epoch 23 Batch 40 Loss 0.1328\n",
            "Epoch 23 Batch 50 Loss 0.1262\n",
            "Epoch 23 Loss 0.1218\n",
            "Time taken for 1 epoch 8.300721645355225 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0971\n",
            "Epoch 24 Batch 10 Loss 0.1180\n",
            "Epoch 24 Batch 20 Loss 0.0870\n",
            "Epoch 24 Batch 30 Loss 0.0979\n",
            "Epoch 24 Batch 40 Loss 0.1158\n",
            "Epoch 24 Batch 50 Loss 0.0973\n",
            "Epoch 24 Loss 0.1013\n",
            "Time taken for 1 epoch 8.572189569473267 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0899\n",
            "Epoch 25 Batch 10 Loss 0.0903\n",
            "Epoch 25 Batch 20 Loss 0.0836\n",
            "Epoch 25 Batch 30 Loss 0.0799\n",
            "Epoch 25 Batch 40 Loss 0.0833\n",
            "Epoch 25 Batch 50 Loss 0.1214\n",
            "Epoch 25 Loss 0.0901\n",
            "Time taken for 1 epoch 8.260876417160034 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0896\n",
            "Epoch 26 Batch 10 Loss 0.0890\n",
            "Epoch 26 Batch 20 Loss 0.0940\n",
            "Epoch 26 Batch 30 Loss 0.0865\n",
            "Epoch 26 Batch 40 Loss 0.0810\n",
            "Epoch 26 Batch 50 Loss 0.0853\n",
            "Epoch 26 Loss 0.0836\n",
            "Time taken for 1 epoch 8.586329460144043 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0649\n",
            "Epoch 27 Batch 10 Loss 0.0707\n",
            "Epoch 27 Batch 20 Loss 0.0540\n",
            "Epoch 27 Batch 30 Loss 0.0632\n",
            "Epoch 27 Batch 40 Loss 0.0776\n",
            "Epoch 27 Batch 50 Loss 0.0735\n",
            "Epoch 27 Loss 0.0684\n",
            "Time taken for 1 epoch 8.353598594665527 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0667\n",
            "Epoch 28 Batch 10 Loss 0.0444\n",
            "Epoch 28 Batch 20 Loss 0.0422\n",
            "Epoch 28 Batch 30 Loss 0.0538\n",
            "Epoch 28 Batch 40 Loss 0.0539\n",
            "Epoch 28 Batch 50 Loss 0.0767\n",
            "Epoch 28 Loss 0.0548\n",
            "Time taken for 1 epoch 8.660667181015015 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0436\n",
            "Epoch 29 Batch 10 Loss 0.0455\n",
            "Epoch 29 Batch 20 Loss 0.0777\n",
            "Epoch 29 Batch 30 Loss 0.0429\n",
            "Epoch 29 Batch 40 Loss 0.0492\n",
            "Epoch 29 Batch 50 Loss 0.0532\n",
            "Epoch 29 Loss 0.0450\n",
            "Time taken for 1 epoch 8.291949272155762 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0303\n",
            "Epoch 30 Batch 10 Loss 0.0288\n",
            "Epoch 30 Batch 20 Loss 0.0323\n",
            "Epoch 30 Batch 30 Loss 0.0379\n",
            "Epoch 30 Batch 40 Loss 0.0465\n",
            "Epoch 30 Batch 50 Loss 0.0446\n",
            "Epoch 30 Loss 0.0377\n",
            "Time taken for 1 epoch 8.606394052505493 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtkdE_Z2IaOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b4bcb66-c0cf-452e-bc5a-0ef7ee83c780"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0c5598c780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ndygMaRyxc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint.restore(\"/content/training_checkpoints/ckpt-3\")\n",
        "encoder_old=checkpoint.encoder\n",
        "decoder_old=checkpoint.decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt9313xb4RlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4df0671-44db-40e0-92fc-f0026d0558e6"
      },
      "source": [
        "encoder_old"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Encoder at 0x7f0cb58081d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7O5bIzk-S4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5b106b36-de96-4175-e52e-5dc5884730c0"
      },
      "source": [
        "for example, label in test_data.take(10):\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  input_t = example[0]\n",
        "  output_label = label[0]\n",
        "  enc_out, enc_hidden = encoder(tf.expand_dims(input_t, axis=0), hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([all_en_tokens[\"<start>\"]], 0)\n",
        "\n",
        "  result = \"\"\n",
        "\n",
        "  for t in range(padded_fr_indices.shape[-1]):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                          dec_hidden,\n",
        "                                                          enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    corresponding_word = [word for word, id in all_en_tokens.items() if id==predicted_id]\n",
        "    result += corresponding_word[0] + \" \"\n",
        "\n",
        "    if corresponding_word[0] == '<end>':\n",
        "      break\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  input_sentence = \"\"\n",
        "  for token_id in input_t:\n",
        "    if token_id==0:\n",
        "      break\n",
        "    \n",
        "    corresponding_word = [word for word, id in all_fr_tokens.items() if id==token_id]\n",
        "    input_sentence += corresponding_word[0] + \" \"\n",
        "    if corresponding_word[0] == \"<end>\":\n",
        "      break\n",
        "\n",
        "  true_translation = \"\"\n",
        "  for token_id in output_label:\n",
        "    if token_id==0:\n",
        "      break\n",
        "    corresponding_word = [word for word, id in all_en_tokens.items() if id==token_id]\n",
        "    true_translation += corresponding_word[0] + \" \"\n",
        "    if corresponding_word[0] == \"<end>\":\n",
        "      break \n",
        "\n",
        "\n",
        "print(\"French sentence: {}\".format(input_sentence))\n",
        "print(\"True translation: {}\".format(true_translation))\n",
        "print(\"Model translation: {}\".format(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "French sentence: Qu' est -ce qui se passe , ici ? \n",
            "True translation: <start> What 's the deal here ? <end> \n",
            "Model translation: What 's your way ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xubxNmzIHnI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27c2d735-3fdb-4070-fa6a-411e9fdcf134"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut4jQes1zIxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a9e0a3b1-dc45-446f-a2b9-0acfc775f774"
      },
      "source": [
        "for example, label in test_data.take(10):\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  input_t = example[0]\n",
        "  output_label = label[0]\n",
        "  enc_out, enc_hidden = encoder_old(tf.expand_dims(input_t, axis=0), hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([all_en_tokens[\"<start>\"]], 0)\n",
        "\n",
        "  result = \"\"\n",
        "\n",
        "  for t in range(padded_fr_indices.shape[-1]):\n",
        "    predictions, dec_hidden, attention_weights = decoder_old(dec_input,\n",
        "                                                          dec_hidden,\n",
        "                                                          enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    corresponding_word = [word for word, id in all_en_tokens.items() if id==predicted_id]\n",
        "    result += corresponding_word[0] + \" \"\n",
        "\n",
        "    if corresponding_word[0] == '<end>':\n",
        "      break\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  input_sentence = \"\"\n",
        "  for token_id in input_t:\n",
        "    \n",
        "    corresponding_word = [word for word, id in all_fr_tokens.items() if id==token_id]\n",
        "    input_sentence += corresponding_word[0] + \" \"\n",
        "    if corresponding_word[0] == \"<end>\":\n",
        "      break\n",
        "\n",
        "  true_translation = \"\"\n",
        "  for token_id in output_label:\n",
        "    corresponding_word = [word for word, id in all_en_tokens.items() if id==token_id]\n",
        "    true_translation += corresponding_word[0] + \" \"\n",
        "    if corresponding_word[0] == \"<end>\":\n",
        "      break \n",
        "\n",
        "\n",
        "print(\"French sentence: {}\".format(input_sentence))\n",
        "print(\"True translation: {}\".format(true_translation))\n",
        "print(\"Model translation: {}\".format(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "French sentence: <start> Je veux t' allouer suffisamment de temps pour faire ça . <end> \n",
            "True translation: <start> I want to give you enough time to do that . <end> \n",
            "Model translation: I want you want to do . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZMyM88_JPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}